{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Python program that combines the correct sentences from the two lists to form the correct sentences (*make your best guess :)*) and prints them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = [\n",
    " 'Outlier or anomaly detection is used', #0\n",
    " 'Principal Component Analysis (PCA) is a method', \n",
    " 'Principal Component Analysis (PCA) is frequently used',\n",
    " 'There are many clustering methods', #3\n",
    " 'Non-negative Matrix Factorization (NMF) is an algorithm',\n",
    " 'Most clustering algorithms are based on a distance metric', #5\n",
    " 'Gaussian Mixture Models (GMM) are a generative model',\n",
    " 'Unsupervised learning is a family of machine learning methods',\n",
    " 'The \"curse of dimensionality\" is a problem', #8\n",
    " 't-SNE reduces data to two dimensions'#9\n",
    "]\n",
    "\n",
    "right = [\n",
    " 'frequently used in recommender systems and customer segmentation.',\n",
    " 'to visualize complex datasets.', #9\n",
    " 'that becomes worse the more features you have.', #8\n",
    " 'to identify credit card fraud.',\n",
    " 'for dimensionality reduction.',\n",
    " 'e.g. Euclidean or Manhatten distance.', #5\n",
    " 'that do not require labled data.',\n",
    " 'for detecting outliers.', #0\n",
    " 'as part of a supervised learning pipeline.',\n",
    " 'like K-means, DBSCAN or Ward.' #3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier or anomaly detection is used to identify credit card fraud.\n",
      "\n",
      "Principal Component Analysis (PCA) is a method for dimensionality reduction.\n",
      "\n",
      "Principal Component Analysis (PCA) is frequently used as part of a supervised learning pipeline.\n",
      "\n",
      "There are many clustering methods like K-means, DBSCAN or Ward.\n",
      "\n",
      "Non-negative Matrix Factorization (NMF) is an algorithm frequently used in recommender systems and customer segmentation.\n",
      "\n",
      "Most clustering algorithms are based on a distance metric e.g. Euclidean or Manhatten distance.\n",
      "\n",
      "Gaussian Mixture Models (GMM) are a generative model for detecting outliers.\n",
      "\n",
      "Unsupervised learning is a family of machine learning methods that do not require labled data.\n",
      "\n",
      "The \"curse of dimensionality\" is a problem that becomes worse the more features you have.\n",
      "\n",
      "t-SNE reduces data to two dimensions to visualize complex datasets.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Positions = [(0, 3), (1, 4), (2, 8), (3, 9), (4,0), (5,5), (6, 7), (7, 6), (8, 2), (9,1)]\n",
    "             \n",
    "for a, b in Positions:\n",
    "    print(left[a], right[b])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 methods of Unsupervised Learning\n",
    "\n",
    "1. **PCA - Principal Component Analysis**: It takes a dataset with M features and reduces this to a dataset with N features, where N is smaller or equal to M. \n",
    "    - It uses the concept of Matrix reduction e.g. eigenvalues and eigenvectors\n",
    "    - Dimensionality Reduction is what is performed! \n",
    "    - eg 100 to 10 features, why would it be useful? \n",
    "        - You might want your features to be independent of each other to improve your model\n",
    "        - Throwing in too many features can make your model slow\n",
    "        - could help with overfitting\n",
    "        \n",
    "        \n",
    "2. **NMF - Non-negative matrix factorization**: Different to PCA as PCA chooses the most important features first - it ranks them. But in NMF all these features are equal in the sense that they have no ranking. NMF also uses dimensionality reduction:\n",
    "    - for segmentation \n",
    "    - Application: Recommenders\n",
    "    - They both give you a linear transformation\n",
    "\n",
    "\n",
    "3. **t-SNE - t-distributed stochastic neighbor embedding**: It takes a dataset with M dimensions and puts it into 2 dimesions. t-SNE should be performed with M features less than or equal to 20, more than that makes it slow and/or bad!\n",
    "    - It is essentially a data visualisation method\n",
    "    - With 2 dimensions you can still see a lot, so 3-D isn't really necessary\n",
    "    - tries to give you visually separate groups of the data\n",
    "    \n",
    "\n",
    "4. **Clustering - k-Means, DBSCAN, Agglomerative etc** - tries to group: M features are clustered into 1 category\n",
    "    - Clustering is something we can use for segmentation\n",
    "    - Also used to visualise our data\n",
    "    \n",
    "\n",
    "5. **GMM - Gaussian Modelling**: A model we can use for anomaly detection! Uses binary - is our point 0 or 1? \n",
    "    - It detects outliers! \n",
    "    - It is an extreme type of clustering method.\n",
    "    - used for credit card and insurance fraud detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
